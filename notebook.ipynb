{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import torch.cuda as cuda\n",
    "\n",
    "class HandwrittenOCR:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the TrOCR model for handwritten text recognition\"\"\"\n",
    "        self.model_name = \"microsoft/trocr-base-handwritten\"\n",
    "\n",
    "        # Set up device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == 'cuda':\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        else:\n",
    "            print(\"GPU not available, using CPU\")\n",
    "\n",
    "        # Load model and processor\n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "        self.processor = TrOCRProcessor.from_pretrained(self.model_name)\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(self.model_name)\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Optional: Enable half precision for faster processing on GPU\n",
    "        if self.device.type == 'cuda':\n",
    "            self.model.half()  # Convert to FP16\n",
    "\n",
    "    def preprocess_image(self, image: Image) -> torch.Tensor:\n",
    "        \"\"\"Preprocess image for model input\"\"\"\n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Process image\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Move to GPU and convert to half precision if using GPU\n",
    "        pixel_values = pixel_values.to(self.device)\n",
    "        if self.device.type == 'cuda':\n",
    "            pixel_values = pixel_values.half()\n",
    "\n",
    "        return pixel_values\n",
    "\n",
    "    def process_single_image(self, image_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single image and return the results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path)\n",
    "            pixel_values = self.preprocess_image(image)\n",
    "\n",
    "            # Generate text\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                generated_ids = self.model.generate(pixel_values)\n",
    "                text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "            process_time = time.time() - start_time\n",
    "\n",
    "            return {\n",
    "                'text': text.strip(),\n",
    "                'processing_time': process_time,\n",
    "                'status': 'success'\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': f\"Error: {str(e)}\",\n",
    "                'processing_time': 0,\n",
    "                'status': 'error'\n",
    "            }\n",
    "\n",
    "    def process_batch(self, image_paths: List[str], batch_size: int = 4) -> Dict:\n",
    "        \"\"\"\n",
    "        Process multiple images in batches\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Process images in batches\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_images = []\n",
    "            valid_paths = []\n",
    "\n",
    "            # Load batch images\n",
    "            for img_path in batch_paths:\n",
    "                try:\n",
    "                    image = Image.open(img_path)\n",
    "                    batch_images.append(image)\n",
    "                    valid_paths.append(img_path)\n",
    "                except Exception as e:\n",
    "                    results[img_path] = {\n",
    "                        'text': f\"Error loading image: {str(e)}\",\n",
    "                        'processing_time': 0,\n",
    "                        'status': 'error'\n",
    "                    }\n",
    "\n",
    "            if not batch_images:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Preprocess batch\n",
    "                inputs = self.processor(batch_images, return_tensors=\"pt\", padding=True)\n",
    "                pixel_values = inputs.pixel_values.to(self.device)\n",
    "                if self.device.type == 'cuda':\n",
    "                    pixel_values = pixel_values.half()\n",
    "\n",
    "                # Generate text\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(pixel_values)\n",
    "                    texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                process_time = time.time() - start_time\n",
    "\n",
    "                # Store results\n",
    "                for img_path, text in zip(valid_paths, texts):\n",
    "                    results[img_path] = {\n",
    "                        'text': text.strip(),\n",
    "                        'processing_time': process_time / len(batch_images),\n",
    "                        'status': 'success'\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Batch processing error: {str(e)}\"\n",
    "                for img_path in valid_paths:\n",
    "                    results[img_path] = {\n",
    "                        'text': error_msg,\n",
    "                        'processing_time': 0,\n",
    "                        'status': 'error'\n",
    "                    }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict, output_file: str = 'handwritten_ocr_results.txt'):\n",
    "        \"\"\"Save results to file\"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Handwritten Text Recognition Results\\n\")\n",
    "            f.write(f\"Model: {self.model_name}\\n\")\n",
    "            f.write(f\"Device: {self.device}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "            # Calculate statistics\n",
    "            successful = sum(1 for r in results.values() if r['status'] == 'success')\n",
    "            total_time = sum(r['processing_time'] for r in results.values() if r['status'] == 'success')\n",
    "\n",
    "            # Write summary\n",
    "            f.write(f\"Summary:\\n\")\n",
    "            f.write(f\"Total images processed: {len(results)}\\n\")\n",
    "            f.write(f\"Successful: {successful}\\n\")\n",
    "            f.write(f\"Failed: {len(results) - successful}\\n\")\n",
    "            f.write(f\"Total processing time: {total_time:.2f} seconds\\n\")\n",
    "            f.write(f\"Average time per image: {total_time/successful:.2f} seconds\\n\\n\")\n",
    "\n",
    "            # Write individual results\n",
    "            for img_path, result in results.items():\n",
    "                f.write(\"-\" * 60 + \"\\n\")\n",
    "                f.write(f\"Image: {img_path}\\n\")\n",
    "                f.write(f\"Status: {result['status']}\\n\")\n",
    "                f.write(f\"Text: {result['text']}\\n\")\n",
    "                if result['status'] == 'success':\n",
    "                    f.write(f\"Processing time: {result['processing_time']:.2f} seconds\\n\")\n",
    "\n",
    "def main():\n",
    "    # Initialize OCR\n",
    "    ocr = HandwrittenOCR()\n",
    "\n",
    "    # List of images to process\n",
    "    image_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']\n",
    "\n",
    "    # Choose processing mode (batch or single)\n",
    "    USE_BATCH = True\n",
    "\n",
    "    if USE_BATCH:\n",
    "        print(\"\\nProcessing images in batch mode...\")\n",
    "        results = ocr.process_batch(image_paths, batch_size=2)\n",
    "    else:\n",
    "        print(\"\\nProcessing images individually...\")\n",
    "        results = {}\n",
    "        for img_path in image_paths:\n",
    "            print(f\"\\nProcessing {img_path}\")\n",
    "            results[img_path] = ocr.process_single_image(img_path)\n",
    "\n",
    "    # Save results\n",
    "    ocr.save_results(results)\n",
    "    print(\"\\nProcessing complete. Results saved to handwritten_ocr_results.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "class GPUAcceleratedOCR:\n",
    "    def __init__(self, lang='eng'):\n",
    "        \"\"\"\n",
    "        Initialize OCR with GPU support and enhanced image processing\n",
    "        \"\"\"\n",
    "        self.lang = lang\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Enhanced transforms for better preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "        ])\n",
    "\n",
    "    def enhance_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply advanced image enhancement techniques\n",
    "        \"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image.copy()\n",
    "\n",
    "        # Create CLAHE object for contrast enhancement\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        enhanced = clahe.apply(gray)\n",
    "\n",
    "        # Denoise using GPU if available\n",
    "        if self.device.type == 'cuda':\n",
    "            # Convert to tensor for GPU processing\n",
    "            img_tensor = torch.from_numpy(enhanced).float().unsqueeze(0).unsqueeze(0)\n",
    "            img_tensor = img_tensor.to(self.device)\n",
    "\n",
    "            # Apply GPU-accelerated denoising\n",
    "            with torch.no_grad():\n",
    "                # Bilateral filtering simulation using separable convolutions\n",
    "                gaussian_kernel = torch.tensor([\n",
    "                    [1, 4, 6, 4, 1],\n",
    "                    [4, 16, 24, 16, 4],\n",
    "                    [6, 24, 36, 24, 6],\n",
    "                    [4, 16, 24, 16, 4],\n",
    "                    [1, 4, 6, 4, 1]\n",
    "                ], dtype=torch.float32, device=self.device) / 256\n",
    "\n",
    "                gaussian_kernel = gaussian_kernel.view(1, 1, 5, 5)\n",
    "                img_tensor = torch.nn.functional.conv2d(\n",
    "                    img_tensor,\n",
    "                    gaussian_kernel,\n",
    "                    padding=2\n",
    "                )\n",
    "\n",
    "                denoised = img_tensor.squeeze().cpu().numpy()\n",
    "        else:\n",
    "            # CPU fallback for denoising\n",
    "            denoised = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
    "\n",
    "        # Convert back to uint8\n",
    "        denoised = (denoised * 255).astype(np.uint8) if denoised.dtype != np.uint8 else denoised\n",
    "\n",
    "        return denoised\n",
    "\n",
    "    def deskew_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Deskew image using GPU acceleration if available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to binary\n",
    "            thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "            # Find all non-zero points\n",
    "            coords = np.column_stack(np.where(thresh > 0))\n",
    "\n",
    "            if len(coords) < 20:  # Not enough points to determine angle\n",
    "                return image\n",
    "\n",
    "            if self.device.type == 'cuda':\n",
    "                # GPU accelerated angle calculation\n",
    "                coords_tensor = torch.from_numpy(coords).float().to(self.device)\n",
    "\n",
    "                # Calculate angle using PCA\n",
    "                with torch.no_grad():\n",
    "                    mean = torch.mean(coords_tensor, dim=0)\n",
    "                    centered_coords = coords_tensor - mean\n",
    "                    cov = torch.mm(centered_coords.t(), centered_coords)\n",
    "                    _, eigenvectors = torch.linalg.eigh(cov)\n",
    "                    angle = torch.atan2(eigenvectors[-1, 1], eigenvectors[-1, 0])\n",
    "                    angle = torch.rad2deg(angle).cpu().numpy()\n",
    "            else:\n",
    "                # CPU fallback\n",
    "                angle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "            # Determine if angle needs to be adjusted\n",
    "            if angle < -45:\n",
    "                angle = 90 + angle\n",
    "            elif angle > 45:\n",
    "                angle = -90 + angle\n",
    "\n",
    "            # Rotate the image\n",
    "            (h, w) = image.shape[:2]\n",
    "            center = (w // 2, h // 2)\n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated = cv2.warpAffine(\n",
    "                image, M, (w, h),\n",
    "                flags=cv2.INTER_CUBIC,\n",
    "                borderMode=cv2.BORDER_REPLICATE\n",
    "            )\n",
    "\n",
    "            return rotated\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Deskewing failed - {str(e)}\")\n",
    "            return image\n",
    "\n",
    "    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing pipeline\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initial enhancement\n",
    "            enhanced = self.enhance_image(image)\n",
    "\n",
    "            # Deskew\n",
    "            deskewed = self.deskew_image(enhanced)\n",
    "\n",
    "            # Convert to tensor for GPU processing\n",
    "            img_tensor = self.transform(deskewed).unsqueeze(0)\n",
    "            img_tensor = img_tensor.to(self.device)\n",
    "\n",
    "            # GPU-accelerated processing\n",
    "            with torch.no_grad():\n",
    "                # Additional denoising\n",
    "                img_tensor = torch.nn.functional.avg_pool2d(\n",
    "                    img_tensor, 2, stride=1, padding=1\n",
    "                )\n",
    "\n",
    "                # Enhance edges\n",
    "                kernel = torch.tensor([\n",
    "                    [-1, -1, -1],\n",
    "                    [-1,  9, -1],\n",
    "                    [-1, -1, -1]\n",
    "                ], dtype=torch.float32, device=self.device).view(1, 1, 3, 3)\n",
    "                img_tensor = torch.nn.functional.conv2d(\n",
    "                    img_tensor, kernel, padding=1\n",
    "                )\n",
    "\n",
    "                # Normalize\n",
    "                img_tensor = torch.nn.functional.normalize(img_tensor, dim=1)\n",
    "\n",
    "                # Convert back to numpy\n",
    "                processed = (img_tensor.squeeze().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "            # Final CPU operations\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "            processed = cv2.dilate(processed, kernel, iterations=1)\n",
    "            processed = cv2.medianBlur(processed, 3)\n",
    "\n",
    "            return processed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Advanced preprocessing failed - {str(e)}\")\n",
    "            # Fallback to basic preprocessing\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    def image_to_text(self, image_path: str, preprocess: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract text from image with GPU acceleration\n",
    "        Args:\n",
    "            image_path: Path to image file\n",
    "            preprocess: Whether to apply preprocessing\n",
    "        Returns:\n",
    "            Dictionary containing extracted text and confidence\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not read image at {image_path}\")\n",
    "\n",
    "            # Preprocess\n",
    "            if preprocess:\n",
    "                image = self.preprocess_image(image)\n",
    "\n",
    "            # Extract text\n",
    "            custom_config = r'--oem 3 --psm 6'\n",
    "            text = pytesseract.image_to_string(image, lang=self.lang, config=custom_config)\n",
    "\n",
    "            # Get confidence scores\n",
    "            data = pytesseract.image_to_data(image, lang=self.lang, config=custom_config,\n",
    "                                           output_type=pytesseract.Output.DICT)\n",
    "            conf_scores = [int(conf) for conf in data['conf'] if conf != '-1']\n",
    "            confidence = sum(conf_scores) / len(conf_scores) if conf_scores else 0\n",
    "\n",
    "            process_time = time.time() - start_time\n",
    "\n",
    "            return {\n",
    "                'text': text.strip(),\n",
    "                'confidence': confidence,\n",
    "                'process_time': process_time\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return {\n",
    "                'text': f\"Error: {str(e)}\",\n",
    "                'confidence': 0,\n",
    "                'process_time': 0\n",
    "            }\n",
    "\n",
    "    def process_batch(self, image_paths: List[str], batch_size: int = 4) -> Dict:\n",
    "        \"\"\"\n",
    "        Process multiple images in batches using GPU\n",
    "        Args:\n",
    "            image_paths: List of image paths\n",
    "            batch_size: Number of images to process simultaneously\n",
    "        Returns:\n",
    "            Dictionary of results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "\n",
    "            # Process batch\n",
    "            for img_path in batch_paths:\n",
    "                print(f\"\\nProcessing {img_path}\")\n",
    "                result = self.image_to_text(img_path)\n",
    "                results[img_path] = result\n",
    "\n",
    "                print(f\"Detected text: {result['text']}\")\n",
    "                print(f\"Confidence: {result['confidence']:.2f}%\")\n",
    "                print(f\"Processing time: {result['process_time']:.3f} seconds\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict, output_file: str = 'ocr_results.txt'):\n",
    "        \"\"\"\n",
    "        Save OCR results to file\n",
    "        Args:\n",
    "            results: Dictionary of OCR results\n",
    "            output_file: Output file path\n",
    "        \"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"OCR Results (Using {self.device})\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            total_time = sum(result['process_time'] for result in results.values())\n",
    "            avg_confidence = sum(result['confidence'] for result in results.values()) / len(results)\n",
    "\n",
    "            f.write(f\"Summary:\\n\")\n",
    "            f.write(f\"Total images processed: {len(results)}\\n\")\n",
    "            f.write(f\"Average confidence: {avg_confidence:.2f}%\\n\")\n",
    "            f.write(f\"Total processing time: {total_time:.3f} seconds\\n\")\n",
    "            f.write(f\"Average time per image: {total_time/len(results):.3f} seconds\\n\\n\")\n",
    "\n",
    "            for img_path, result in results.items():\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                f.write(f\"Image: {img_path}\\n\")\n",
    "                f.write(f\"Text: {result['text']}\\n\")\n",
    "                f.write(f\"Confidence: {result['confidence']:.2f}%\\n\")\n",
    "                f.write(f\"Processing time: {result['process_time']:.3f} seconds\\n\")\n",
    "\n",
    "def main():\n",
    "    # Initialize OCR\n",
    "    ocr = GPUAcceleratedOCR(lang='eng')\n",
    "\n",
    "    # List of images to process\n",
    "    image_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg', 'img4.jpg', 'img5.jpg']\n",
    "\n",
    "    # Process images in batches\n",
    "    results = ocr.process_batch(image_paths, batch_size=2)\n",
    "\n",
    "    # Save results\n",
    "    ocr.save_results(results)\n",
    "    print(\"\\nProcessing complete. Results saved to ocr_results.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MedicationNER:\n",
    "    def __init__(self,api_key: str = None):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the NER system with predefined patterns and rules\n",
    "        Args:\n",
    "            api_key: OpenFDA API key\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        self.entity_types = {\n",
    "            'MEDICATION': self.fetch_medications_from_openfda(),\n",
    "            'DOSAGE': [r'\\d+\\s*(?:mg|mL|tablet|tablets|puffs|grams|drops|capsule)',\n",
    "                      r'\\d+\\s*(?:gram|g|mcg)'],\n",
    "            'STRENGTH': [r'\\d+\\s*(?:mg|mcg|%)',\n",
    "                        r'\\d+\\.\\d+\\s*(?:mg|mcg|%)'],\n",
    "            'ROUTE': ['orally', 'topically', 'intramuscularly', 'inhale',\n",
    "                     'via inhaler', 'into each eye'],\n",
    "            'FREQUENCY': ['every 8 hours', 'twice daily', 'three times a day',\n",
    "                         'once daily', 'single dose', 'every 12 hours',\n",
    "                         'every 4 to 6 hours'],\n",
    "            'DURATION': [r'\\d+\\s*(?:days|weeks|months)',\n",
    "                        r'\\d+\\s*(?:day|week|month)'],\n",
    "            'FORM': ['cream', 'syrup', 'ophthalmic solution', 'inhaler', 'tablet',\n",
    "                    'capsule'],\n",
    "            'ADMINISTRATION_SITE': ['affected area', 'into each eye'],\n",
    "            'ADMINISTRATION_TIME': ['at bedtime', 'immediately'],\n",
    "            'ADMINISTRATION_CONDITION': ['after meals', 'with the evening meal',\n",
    "                                       'as needed for wheezing']\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def fetch_medications_from_openfda(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Fetch a list of medication names from the openFDA API.\n",
    "        Returns:\n",
    "            A list of medication generic and brand names.\n",
    "        \"\"\"\n",
    "        medications = set()\n",
    "        base_url = \"https://api.fda.gov/drug/ndc.json\"\n",
    "        # Initialize parameters\n",
    "        limit = 1000  # Maximum results per request\n",
    "        total_requests = 5  # Limit number of requests to avoid rate limiting\n",
    "\n",
    "        for skip in range(0, limit * total_requests, limit):\n",
    "            params = {\n",
    "                'search': 'product_type:\"HUMAN PRESCRIPTION DRUG\"',\n",
    "                'limit': limit,\n",
    "                'skip': skip,\n",
    "                'api_key': self.api_key  # Add your API key here\n",
    "            }\n",
    "\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if 'results' in data:\n",
    "                for result in data['results']:\n",
    "                    # Add generic name if available\n",
    "                    if 'generic_name' in result:\n",
    "                        generic_names = result['generic_name'].split(', ')\n",
    "                        medications.update(generic_names)\n",
    "\n",
    "                    # Add brand name if available\n",
    "                    if 'brand_name' in result:\n",
    "                        brand_names = result['brand_name'].split(', ')\n",
    "                        medications.update(brand_names)\n",
    "\n",
    "                    # Add active ingredients if available\n",
    "                    if 'active_ingredients' in result:\n",
    "                        for ingredient in result['active_ingredients']:\n",
    "                            if 'name' in ingredient:\n",
    "                                medications.add(ingredient['name'])\n",
    "\n",
    "\n",
    "        # Clean up medication names\n",
    "        cleaned_medications = set()\n",
    "        for med in medications:\n",
    "            # Convert to title case and strip whitespace\n",
    "            cleaned_med = med.title().strip()\n",
    "            # Remove entries that are too short or contain unwanted characters\n",
    "            if len(cleaned_med) > 2 and cleaned_med.replace(' ', '').isalnum():\n",
    "                cleaned_medications.add(cleaned_med)\n",
    "\n",
    "        return list(cleaned_medications)\n",
    "\n",
    "\n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract medication-related entities from text\n",
    "\n",
    "        Args:\n",
    "            text: Input medication instruction\n",
    "        Returns:\n",
    "            List of detected entities with their types\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "\n",
    "        # Process each entity type\n",
    "        for entity_type, patterns in self.entity_types.items():\n",
    "            for pattern in patterns:\n",
    "                # Look for exact matches first\n",
    "                if pattern in text:\n",
    "                    start = text.find(pattern)\n",
    "                    entities.append({\n",
    "                        'text': pattern,\n",
    "                        'type': entity_type,\n",
    "                        'start': start,\n",
    "                        'end': start + len(pattern)\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Look for pattern matches\n",
    "                import re\n",
    "                matches = re.finditer(pattern, text)\n",
    "                for match in matches:\n",
    "                    entities.append({\n",
    "                        'text': match.group(),\n",
    "                        'type': entity_type,\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end()\n",
    "                    })\n",
    "\n",
    "        # Sort entities by start position\n",
    "        entities = sorted(entities, key=lambda x: x['start'])\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def process_and_format(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Process text and return formatted output\n",
    "\n",
    "        Args:\n",
    "            text: Input medication instruction\n",
    "        Returns:\n",
    "            Formatted string with detected entities\n",
    "        \"\"\"\n",
    "        entities = self.extract_entities(text)\n",
    "\n",
    "        output = f\"Sentence: \\\"{text}\\\"\\n\"\n",
    "        output += \"Entities:\\n\"\n",
    "\n",
    "        # Group entities by type\n",
    "        grouped_entities = {}\n",
    "        for entity in entities:\n",
    "            if entity['type'] not in grouped_entities:\n",
    "                grouped_entities[entity['type']] = []\n",
    "            grouped_entities[entity['type']].append(entity['text'])\n",
    "\n",
    "        # Format output\n",
    "        for entity_type, entity_texts in grouped_entities.items():\n",
    "            for text in entity_texts:\n",
    "                output += f\"  * {text}: {entity_type}\\n\"\n",
    "\n",
    "        return output\n",
    "def save_results_to_file(examples, results, filename='medication_analysis_results.txt'):\n",
    "    \"\"\"\n",
    "    Save the analysis results to a text file with clear formatting\n",
    "\n",
    "    Args:\n",
    "        examples: List of example sentences\n",
    "        results: List of processed results\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Write header\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"MEDICATION PRESCRIPTION ANALYSIS RESULTS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        # Write summary\n",
    "        f.write(f\"Total Prescriptions Analyzed: {len(examples)}\\n\")\n",
    "        f.write(\"Analysis Date: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\")\n",
    "\n",
    "        # Write detailed results\n",
    "        for i, (example, result) in enumerate(zip(examples, results), 1):\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            f.write(f\"PRESCRIPTION #{i}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            # Original text\n",
    "            f.write(\"Original Prescription:\\n\")\n",
    "            f.write(f\"\\\"{example}\\\"\\n\\n\")\n",
    "\n",
    "            # Extracted entities\n",
    "            f.write(\"Extracted Entities:\\n\")\n",
    "            entities = result.split('Entities:\\n')[1].strip().split('\\n')\n",
    "\n",
    "            # Group entities by type\n",
    "            entity_dict = {}\n",
    "            for entity in entities:\n",
    "                if entity.strip():\n",
    "                    text, etype = entity.strip()[2:].split(': ')\n",
    "                    if etype not in entity_dict:\n",
    "                        entity_dict[etype] = []\n",
    "                    entity_dict[etype].append(text)\n",
    "\n",
    "            # Write grouped entities in order of importance\n",
    "            entity_order = [\n",
    "                'MEDICATION', 'DOSAGE', 'STRENGTH', 'FORM', 'ROUTE',\n",
    "                'FREQUENCY', 'DURATION', 'ADMINISTRATION_SITE',\n",
    "                'ADMINISTRATION_TIME', 'ADMINISTRATION_CONDITION'\n",
    "            ]\n",
    "\n",
    "            for etype in entity_order:\n",
    "                if etype in entity_dict:\n",
    "                    f.write(f\"\\n{etype}:\\n\")\n",
    "                    for text in entity_dict[etype]:\n",
    "                        f.write(f\"  • {text}\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "def main():\n",
    "    # Initialize NER\n",
    "    api_key = userdata.get('OPENFDA_KEY')\n",
    "    ner = MedicationNER(api_key)\n",
    "\n",
    "    # Example medication instructions\n",
    "    examples = [\n",
    "        \"Administer 500 mg of Amoxicillin orally every 8 hours for 7 days.\",\n",
    "        \"Prescribe 2 tablets of Ibuprofen 200 mg to be taken twice daily after meals.\",\n",
    "        \"Apply 1% Hydrocortisone cream topically to the affected area three times a day.\",\n",
    "        \"Take 10 mL of Cetirizine syrup orally once daily at bedtime.\",\n",
    "        \"Inject 0.3 mg of Epinephrine intramuscularly immediately as a single dose.\",\n",
    "        \"Administer 1 tablet of Metformin 500 mg orally with the evening meal.\",\n",
    "        \"Instill 2 drops of Timolol 0.25% ophthalmic solution into each eye twice daily.\",\n",
    "        \"Take 1 capsule of Doxycycline 100 mg orally every 12 hours for 14 days.\",\n",
    "        \"Apply 5 grams of Clotrimazole 1% cream topically to the affected area twice daily for 2 weeks.\",\n",
    "        \"Inhale 2 puffs of Albuterol 90 mcg via inhaler every 4 to 6 hours as needed for wheezing.\"\n",
    "    ]\n",
    "\n",
    "    # Process examples and collect results\n",
    "    results = []\n",
    "    for text in examples:\n",
    "        result = ner.process_and_format(text)\n",
    "        results.append(result)\n",
    "        print(f\"\\n{result}\")\n",
    "\n",
    "    # Save detailed results to file\n",
    "    save_results_to_file(examples, results)\n",
    "    print(\"\\nDetailed results have been saved to 'medication_analysis_results.txt'\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from google.colab import userdata\n",
    "class DrugInteractionChecker:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        Initialize the drug interaction checker\n",
    "        Args:\n",
    "            api_key: OpenFDA API key (required)\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.fda.gov/drug/event.json\"\n",
    "\n",
    "    def extract_medications_from_file(self, filename: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract medication names from the analysis results file\n",
    "        \"\"\"\n",
    "        medications = []\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Find all medication sections\n",
    "            sections = content.split('PRESCRIPTION #')\n",
    "            for section in sections:\n",
    "                if 'MEDICATION:' in section:\n",
    "                    med_section = section.split('MEDICATION:')[1].split('\\n')[1]\n",
    "                    medication = med_section.strip('• ').strip()\n",
    "                    if medication:\n",
    "                        medications.append(medication)\n",
    "\n",
    "            return list(set(medications))  # Remove duplicates\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def get_drug_events(self, drug_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get drug event information from OpenFDA\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'search': f'patient.drug.medicinalproduct:{drug_name}',\n",
    "                'limit': 100,\n",
    "                'api_key': self.api_key\n",
    "            }\n",
    "\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if 'results' not in data:\n",
    "                return {'error': 'No results found'}\n",
    "\n",
    "            # Process and summarize the events\n",
    "            events_summary = self._process_events(data['results'])\n",
    "            return events_summary\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            error_msg = str(e)\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                error_msg = f\"{error_msg}\\nResponse: {e.response.text}\"\n",
    "            return {'error': f'API request failed: {error_msg}'}\n",
    "\n",
    "    def _process_events(self, events: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Process and summarize drug events\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'total_reports': len(events),\n",
    "            'reactions': {},\n",
    "            'interactions': [],\n",
    "            'common_indications': set(),\n",
    "            'warnings': set()\n",
    "        }\n",
    "\n",
    "        for event in events:\n",
    "            if 'patient' in event:\n",
    "                patient = event['patient']\n",
    "\n",
    "                # Process reactions\n",
    "                if 'reaction' in patient:\n",
    "                    for reaction in patient['reaction']:\n",
    "                        if 'reactionmeddrapt' in reaction:\n",
    "                            reaction_term = reaction['reactionmeddrapt']\n",
    "                            summary['reactions'][reaction_term] = summary['reactions'].get(reaction_term, 0) + 1\n",
    "\n",
    "                # Process drug information\n",
    "                if 'drug' in patient:\n",
    "                    for drug in patient['drug']:\n",
    "                        # Get indications\n",
    "                        if 'drugindication' in drug:\n",
    "                            summary['common_indications'].add(drug['drugindication'])\n",
    "\n",
    "                        # Look for interactions\n",
    "                        if drug.get('drugcharacterization') == '2':  # Concomitant medications\n",
    "                            if 'medicinalproduct' in drug:\n",
    "                                summary['interactions'].append(drug['medicinalproduct'])\n",
    "\n",
    "        # Convert sets to lists for JSON serialization\n",
    "        summary['common_indications'] = list(summary['common_indications'])\n",
    "        summary['warnings'] = list(summary['warnings'])\n",
    "\n",
    "        # Sort reactions by frequency\n",
    "        summary['reactions'] = dict(sorted(\n",
    "            summary['reactions'].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        ))\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def save_interaction_results(self, results: Dict, filename: str = 'drug_safety_report.txt'):\n",
    "        \"\"\"\n",
    "        Save drug safety and interaction results to a file\n",
    "        \"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"DRUG SAFETY AND INTERACTION REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Medications Analyzed: {len(results)}\\n\\n\")\n",
    "\n",
    "            for med_name, data in results.items():\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                f.write(f\"MEDICATION: {med_name}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "                if 'error' in data:\n",
    "                    f.write(f\"Error: {data['error']}\\n\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Total reports\n",
    "                f.write(f\"Total Reports Analyzed: {data['total_reports']}\\n\\n\")\n",
    "\n",
    "                # Common reactions\n",
    "                f.write(\"TOP REPORTED REACTIONS:\\n\")\n",
    "                for reaction, count in list(data['reactions'].items())[:10]:  # Top 10\n",
    "                    f.write(f\"• {reaction}: {count} reports\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                # Drug interactions\n",
    "                if data['interactions']:\n",
    "                    f.write(\"REPORTED CONCURRENT MEDICATIONS:\\n\")\n",
    "                    for interaction in set(data['interactions']):\n",
    "                        f.write(f\"• {interaction}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "                # Common indications\n",
    "                if data['common_indications']:\n",
    "                    f.write(\"COMMON INDICATIONS:\\n\")\n",
    "                    for indication in data['common_indications']:\n",
    "                        f.write(f\"• {indication}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"\\nDISCLAIMER:\\n\")\n",
    "            f.write(\"This report is based on FDA adverse event reporting data and should not be used\\n\")\n",
    "            f.write(\"to make medical decisions. Consult healthcare professionals for medical advice.\\n\")\n",
    "\n",
    "def main():\n",
    "    # Initialize checker with API key\n",
    "    API_KEY = userdata.get('OPENFDA_KEY')\n",
    "    checker = DrugInteractionChecker(api_key=API_KEY)\n",
    "\n",
    "    # Extract medications from analysis results\n",
    "    print(\"Extracting medications from analysis results...\")\n",
    "    medications = checker.extract_medications_from_file('medication_analysis_results.txt')\n",
    "\n",
    "    if not medications:\n",
    "        print(\"No medications found in the analysis file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(medications)} medications:\")\n",
    "    for med in medications:\n",
    "        print(f\"• {med}\")\n",
    "\n",
    "    # Check drug events and interactions\n",
    "    print(\"\\nGathering drug safety information...\")\n",
    "    results = {}\n",
    "    for med in medications:\n",
    "        print(f\"Processing {med}...\")\n",
    "        results[med] = checker.get_drug_events(med)\n",
    "\n",
    "    # Save results\n",
    "    checker.save_interaction_results(results)\n",
    "    print(\"\\nDrug safety report has been saved to 'drug_safety_report.txt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def read_prescription(image_path, save_dir='prescriptions_output'):\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Initialize the OCR reader\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Basic preprocessing\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        denoised = cv2.fastNlMeansDenoising(gray)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        enhanced = clahe.apply(denoised)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(enhanced)\n",
    "\n",
    "        # Extract and format the results\n",
    "        prescription_text = []\n",
    "        for (bbox, text, prob) in results:\n",
    "            if prob > 0.3:\n",
    "                prescription_text.append({\n",
    "                    'text': text,\n",
    "                    'confidence': round(prob * 100, 2),\n",
    "                    'position': bbox\n",
    "                })\n",
    "\n",
    "        # Generate filename with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_filename = f\"{base_image_name}_{timestamp}.txt\"\n",
    "        output_path = os.path.join(save_dir, output_filename)\n",
    "\n",
    "        # Save results to text file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Prescription OCR Results\\n\")\n",
    "            f.write(f\"Image: {image_path}\\n\")\n",
    "            f.write(f\"Processed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            for item in prescription_text:\n",
    "                f.write(f\"Text: {item['text']}\\n\")\n",
    "                f.write(f\"Confidence: {item['confidence']}%\\n\")\n",
    "                f.write(f\"Position: {item['position']}\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        return prescription_text, output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def display_results(results, output_path):\n",
    "    if results:\n",
    "        print(\"\\nPrescription Details:\")\n",
    "        print(\"-\" * 50)\n",
    "        for item in results:\n",
    "            print(f\"Text: {item['text']}\")\n",
    "            print(f\"Confidence: {item['confidence']}%\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        print(f\"\\nDetailed results have been saved to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"test_prescription.jpg\"\n",
    "\n",
    "    # Process the image and save results\n",
    "    results, output_path = read_prescription(image_path)\n",
    "\n",
    "    # Display results in console\n",
    "    display_results(results, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch accelerate\n",
    "!pip install -U bitsandbytes\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def setup_model():\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load model with lower precision to save memory\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "        device_map=\"auto\",  # Automatically handle device placement\n",
    "        load_in_8bit=True  # Use 8-bit quantization to reduce memory usage\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def process_text(input_text, model, tokenizer, max_length=512):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    # Setup paths\n",
    "    input_file = \"input.txt\"\n",
    "    output_file = \"output.txt\"\n",
    "\n",
    "    try:\n",
    "        # Read input text\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            input_text = f.read()\n",
    "\n",
    "        # Initialize model and tokenizer\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        model, tokenizer = setup_model()\n",
    "\n",
    "        # Process text\n",
    "        print(\"Processing text...\")\n",
    "        response = process_text(input_text, model, tokenizer)\n",
    "\n",
    "        # Write output\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response)\n",
    "\n",
    "        print(f\"Processing complete! Response saved to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
